PL:

Celem pracy jest wykonanie badań głębokich sieci neuronowych generujących obrazy na podstawie opisów sporządzonych w języku naturalnym. Po zaimplementowaniu i wytrenowaniu sieci głębokich zostało wykonane porównanie uzyskanych rezultatów z wynikami działania popularnych modeli sieci.
Aspektem badawczym jest analiza wpływu hiperparametrów elementów stworzonego modelu, zbiorów danych treningowych oraz różnych modeli przetwarzana języka naturalnego na proces warunkowego generowania obrazów.

Model generatora obrazów na podstawie opisów tekstowych jest połączeniem metody dyfuzji wraz z sieciami GAN.

Architekturę tworzą następujące elementów:

- VQ-VAE – wariacyjny autoenkoder wykorzystujący kwantyzację wektorową (ang. Vector Quantized Variational Autoencoder). Pozwala on na modelowanie dyskretnych reprezentacji danych ukrytych, co stanowi kluczową cechę w zadaniu generowania obrazów. Zbudowany jest z enkodera przekształcającego dane wejściowe w reprezentacje niejawne oraz dekodera wykonującego odwrotną operację. Podczas procesu kwantyzacji, ciągłe reprezentacje niejawne są zastępowane najbliższym wektorem ze słownika.
- Dyskryminator – pełni funkcję dodatkowej oceny jakości zrekonstruowanych obrazów przez VQ-VAE, który w tym przypadku można uznać za generator. Aktywuje się on po zadanym przez użytkownika kroku treningu i pomaga w generowaniu ostrzejszych wyników. Tradycyjny dyskryminator najczęściej ocenia cały obraz wyjściowy z generatora, natomiast ten zastosowany w modelu sprawdza prawdopodobieństwo czy próbka jest sztuczna na podstawie poszczególnych jej wycinków.
- Warunkowa sieć U-Net – jest to główny element zaprojektowany do generowania obrazów na podstawie opisów tekstowych wczytanych przez użytkownika. Składa się ona z bloków downsamplingowych, przetwarzania pośredniego i upsamplingowych. Dodatkowo został dodany warunek tekstowy poprzez implementację mechanizmu uwagi – uwaga własna (ang. self-attention) oraz uwaga krzyżowa (ang. cross-attention). Self-attention pozwala modelowi na analizowanie relacji między różnymi częściami obrazu w celu uzyskania lepszej reprezentacji danych. Odbywa się to poprzez przekształcenie i przeprowadzenie próbki przez warstwę normalizującą, a następnie zwracana jest uwagą na różne obiekty na podstawie wzajemnych zależności. Wynik ten jest ponownie łączony z oryginalnym obrazem tworząc wyjściowy tensor. Cross-attention nadaje sieci umiejętność zwracania uwagi na informacje tekstowe. Podobnie jak w poprzednim mechanizmie dane są normalizowane, a następnie osadzenia tekstowe przekazywane są do modułu uwagi, gdzie przekształcane są na reprezentacje kontekstowe. Następnie łączone są z oryginalnymi obrazami tworząc tensory umożliwiające modelowi generowanie obrazów warunkowanych konkretnym opisem. Każdy blok sieci U-net wykorzystuje funkcję aktywacji „SiLU” oraz posiada dodatkowe warstwy liniowe dla osadzeń czasowych i tekstowych.
- Harmonogram liniowego szumu – element wykorzystywany do kontrolowania i regulacji procesu dyfuzji i odszumiania danych wejściowych. Zarządza ilością dodawanego szumu liniowego dla każdego kroku iteracyjnego operacji.
- CLIP – Contrastive Language-Image Pre-training to sieć neuronowa stworzona przez firmę OpenAI do rozumienia zależności między tekstem a obrazem. Trenowana jest na parze danych obraz – tekst. Umożliwia ona modelowi przetwarzanie informacji z obu rodzajów danych. Model składa się z dwóch części:
- CLIPTokenizer – tokenizator przekształcający ciąg tekstowy na listę identyfikatorów, aby następnie zmapować je na odpowiadające im indeksy w słownik, w dalszym kroku oznacza je według kolejności w celu stworzenia spójnego kontekstu.
- CLIPTextModel – część obsługująca przetwarzanie językowe. Przyjmuje on ztokenizowaną sekwencję danych, aby przetworzyć je przez transformator, tworząc złożone relacje i konteksty wewnątrz opisu. W kolejnym kroku generowane są reprezentacje słów uwzględniające ich znaczenie w zdaniach w celu zakodowania ich w formie wektorów cech – inaczej osadzeń tekstowych (ang. embedding) wykorzystując metodę „zero-shot”. Na koniec porównywane są one z reprezentacjami wizualnymi w wspólnej przestrzeni obrazów i tekstu.

Wyniki modelu:

![Bez tytułu](https://github.com/user-attachments/assets/188a2d34-43b8-49dd-83d8-a6c23b153be9)


ENG:

The goal of the thesis is to research deep neural networks that generate images based on descriptions written in natural language. After implementing and training the deep networks, the obtained results were compared with the results of popular network models.
The research aspect is the analysis of the impact of hyperparameters of the elements of the created model, training data sets and various natural language processing models on the process of conditional image generation.

The image generator model based on text descriptions is a combination of the diffusion method and GAN networks.

The architecture consists of the following elements:

- VQ-VAE – a variational autoencoder using vector quantization (Vector Quantized Variational Autoencoder). It allows the modeling of discrete representations of hidden data, which is a key feature in the task of image generation. It is composed of an encoder that transforms input data into implicit representations and a decoder that performs the reverse operation. During the quantization process, continuous implicit representations are replaced with the closest vector from the dictionary.
- Discriminator - serves as an additional assessment of the quality of reconstructed images by VQ-VAE, which in this case can be considered a generator. It activates after a user-set training step and helps generate sharper results. The traditional discriminator most often evaluates the entire output image from the generator, while the one used in the model checks the probability whether the sample is artificial based on its individual sections.
- Conditional U-Net – this is the main element designed to generate images based on text descriptions loaded by the user. It consists of downsampling, intermediate processing and upsampling blocks. Additionally, a text condition was added by implementing an attention mechanism - self-attention and cross-attention. Self-attention allows the model to analyze the relationships between different parts of the image to obtain a better representation of the data. This is done by transforming and running the sample through a normalization layer, and then drawing attention to different objects based on their interdependencies. This result is recombined with the original image to form the output tensor. Cross-attention gives the network the ability to pay attention to textual information. As in the previous mechanism, the data is normalized and then the text embeddings are transferred to the attention module, where they are transformed into contextual representations. They are then combined with the original images to create tensors that enable the model to generate images conditioned by a specific description. Each U-net block uses the "SiLU" activation function and has additional linear layers for time and text embeddings.
- Linear noise scheduler – an element used to control and regulate the process of diffusion and denoising of input data. Manages the amount of linear noise added for each iteration step of the operation.
- CLIP – Contrastive Language-Image Pre-training is a neural network created by OpenAI to understand the relationship between text and image. It is trained on an image-text pair of data. It allows the model to process information from both types of data. The model consists of two parts:
- CLIPTokenizer – a tokenizer that transforms a text string into a list of identifiers, then maps them to their corresponding indexes in a dictionary, and then tags them in order to create a consistent context.
- CLIPTextModel – part supporting language processing. It takes a tokenized sequence of data to process it through a transformer, creating complex relationships and contexts within the description. In the next step, representations of words are generated taking into account their meaning in sentences in order to encode them in the form of feature vectors - otherwise known as text embeddings - using the "zero-shot" method. Finally, they are compared with visual representations in the common space of images and text.

Results:
![Bez tytułu](https://github.com/user-attachments/assets/188a2d34-43b8-49dd-83d8-a6c23b153be9)
