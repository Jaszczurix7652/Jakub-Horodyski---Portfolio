Celem pracy jest wykonanie badań głębokich sieci neuronowych generujących obrazy na podstawie opisów sporządzonych w języku naturalnym. Po zaimplementowaniu i wytrenowaniu sieci głębokich zostało wykonane porównanie uzyskanych rezultatów z wynikami działania popularnych modeli sieci.
Aspektem badawczym jest analiza wpływu hiperparametrów elementów stworzonego modelu, zbiorów danych treningowych oraz różnych modeli przetwarzana języka naturalnego na proces warunkowego generowania obrazów.

Model generatora obrazów na podstawie opisów tekstowych jest połączeniem metody dyfuzji wraz z sieciami GAN.

Architekturę tworzą następujące elementów:

- VQ-VAE – wariacyjny autoenkoder wykorzystujący kwantyzację wektorową (ang. Vector Quantized Variational Autoencoder). Pozwala on na modelowanie dyskretnych reprezentacji danych ukrytych, co stanowi kluczową cechę w zadaniu generowania obrazów. Zbudowany jest z enkodera przekształcającego dane wejściowe w reprezentacje niejawne oraz dekodera wykonującego odwrotną operację. Podczas procesu kwantyzacji, ciągłe reprezentacje niejawne są zastępowane najbliższym wektorem ze słownika.
- Dyskryminator – pełni funkcję dodatkowej oceny jakości zrekonstruowanych obrazów przez VQ-VAE, który w tym przypadku można uznać za generator. Aktywuje się on po zadanym przez użytkownika kroku treningu i pomaga w generowaniu ostrzejszych wyników. Tradycyjny dyskryminator najczęściej ocenia cały obraz wyjściowy z generatora, natomiast ten zastosowany w modelu sprawdza prawdopodobieństwo czy próbka jest sztuczna na podstawie poszczególnych jej wycinków.
- Warunkowa sieć U-Net – jest to główny element zaprojektowany do generowania obrazów na podstawie opisów tekstowych wczytanych przez użytkownika. Składa się ona z bloków downsamplingowych, przetwarzania pośredniego i upsamplingowych. Dodatkowo został dodany warunek tekstowy poprzez implementację mechanizmu uwagi – uwaga własna (ang. self-attention) oraz uwaga krzyżowa (ang. cross-attention). Self-attention pozwala modelowi na analizowanie relacji między różnymi częściami obrazu w celu uzyskania lepszej reprezentacji danych. Odbywa się to poprzez przekształcenie i przeprowadzenie próbki przez warstwę normalizującą, a następnie zwracana jest uwagą na różne obiekty na podstawie wzajemnych zależności. Wynik ten jest ponownie łączony z oryginalnym obrazem tworząc wyjściowy tensor. Cross-attention nadaje sieci umiejętność zwracania uwagi na informacje tekstowe. Podobnie jak w poprzednim mechanizmie dane są normalizowane, a następnie osadzenia tekstowe przekazywane są do modułu uwagi, gdzie przekształcane są na reprezentacje kontekstowe. Następnie łączone są z oryginalnymi obrazami tworząc tensory umożliwiające modelowi generowanie obrazów warunkowanych konkretnym opisem. Każdy blok sieci U-net wykorzystuje funkcję aktywacji „SiLU” oraz posiada dodatkowe warstwy liniowe dla osadzeń czasowych i tekstowych.
- Harmonogram liniowego szumu – element wykorzystywany do kontrolowania i regulacji procesu dyfuzji i odszumiania danych wejściowych. Zarządza ilością dodawanego szumu liniowego dla każdego kroku iteracyjnego operacji.
- CLIP – Contrastive Language-Image Pre-training to sieć neuronowa stworzona przez firmę OpenAI do rozumienia zależności między tekstem a obrazem. Trenowana jest na parze danych obraz – tekst. Umożliwia ona modelowi przetwarzanie informacji z obu rodzajów danych. Model składa się z dwóch części:
- CLIPTokenizer – tokenizator przekształcający ciąg tekstowy na listę identyfikatorów, aby następnie zmapować je na odpowiadające im indeksy w słownik, w dalszym kroku oznacza je według kolejności w celu stworzenia spójnego kontekstu.
- CLIPTextModel – część obsługująca przetwarzanie językowe. Przyjmuje on ztokenizowaną sekwencję danych, aby przetworzyć je przez transformator, tworząc złożone relacje i konteksty wewnątrz opisu. W kolejnym kroku generowane są reprezentacje słów uwzględniające ich znaczenie w zdaniach w celu zakodowania ich w formie wektorów cech – inaczej osadzeń tekstowych (ang. embedding) wykorzystując metodę „zero-shot”. Na koniec porównywane są one z reprezentacjami wizualnymi w wspólnej przestrzeni obrazów i tekstu.

Wyniki modelu:

![Bez tytułu](https://github.com/user-attachments/assets/188a2d34-43b8-49dd-83d8-a6c23b153be9)


